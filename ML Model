//Загрузка и знакомство с данными
import org.apache.spark.sql.Dataset;
val basePath = "/spark";
val raw = spark.read.option("header", "true").option("inferSchema","true").csv(s"$basePath/data/BankChurners.csv");  

//raw.write.csv(s"$basePath/test")

val columns: Array[String] = raw.columns;
val columnsLen: Int = columns.length;
val colsToDrop: Array[String] = columns.slice(columnsLen - 2, columnsLen) :+ columns.head;
val df = raw.drop(colsToDrop: _*);
df.show(5, truncate = false);

df.printSchema;


//Определяем типы колонок
df.dtypes.foreach { dt => println(f"${dt._1}%25s\t${dt._2}") };

df.dtypes.groupBy(_._2).mapValues(_.length).foreach(println);

val numericColumns: Array[String] = df.dtypes.filter(p => p._2.equals("DoubleType") || p._2.equals("IntegerType")).map(_._1);
df.select(numericColumns.map(col): _*).summary().show; //numericColumns.map(col): _*   - передает коллекцию как набор элементов

df.groupBy("Customer_Age").count().orderBy("Customer_Age").show(100);

//Целевая колонка
val dft = df.withColumn("target", when($"Attrition_Flag" === "Existing Customer", 0).otherwise(1));
dft.select("Attrition_Flag", "target").show(5, truncate = false);


//Проверка сбалансированности данных
dft.groupBy("target").count.show;

//Oversampling
val df1 = dft.filter($"target" === 1)
val df0 = dft.filter($"target" === 0)

val df1count = df1.count
val df0count = df0.count
df0count / df1count


val df1Over = df1.withColumn("dummy", explode(lit((1 to (df0count / df1count).toInt).toArray))).drop("dummy")
df1Over.show(10, truncate = false)


val data = df0.unionAll(df1Over)
data.groupBy("target").count.show


//Работа с признаками
//Проверим корреляции числовых признаков

//Получим список всех пар
val numericColumnsPairs = numericColumns.flatMap(f1 => numericColumns.map(f2 => (f1, f2)))

//Вариант 1: DataFrameStatFunctions
val pairs = numericColumnsPairs.filter { p => !p._1.equals(p._2) }.map { p => if (p._1 < p._2) (p._1, p._2) else (p._2, p._1) }.distinct
val corr = pairs.map { p => (p._1, p._2, data.stat.corr(p._1, p._2)) }.filter(p => math.abs(p._3) > 0.6)

corr.sortBy(_._3).reverse.foreach { c => println(f"${c._1}%25s${c._2}%25s\t${c._3}") }

//Вариант 2: Correlation
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.stat.Correlation
import org.apache.spark.ml.linalg.Matrix
import org.apache.spark.sql.Row

val numericAssembler = new VectorAssembler().setInputCols(numericColumns).setOutputCol("features")
val numeric = numericAssembler.transform(data)
val Row(matrix: Matrix) = Correlation.corr(numeric, "features").head


val corr2 = matrix.toArray.zip(numericColumnsPairs).map(cnn => (cnn._2._1, cnn._2._2, cnn._1)).filter(p => math.abs(p._3) > 0.6 && p._3 < 1.0).map { p => if (p._1 < p._2) (p._1, p._2, p._3) else (p._2, p._1, p._3) }.distinct

corr2.sortBy(_._3).reverse.foreach { c => println(f"${c._1}%25s${c._2}%25s\t${c._3}") }
corr.toSet.intersect(corr2.toSet)


//Оставляем числовые колонки с низкой кореляцией
val numericColumnsFinal = numericColumns.diff(corr.map(_._2))



//Категориальные признаки
//Индексируем строковые колонки
import org.apache.spark.ml.feature.StringIndexer

val stringColumns = data.dtypes.filter(_._2.equals("StringType")).map(_._1).filter(!_.equals("Attrition_Flag"))
val stringColumnsIndexed = stringColumns.map(_ + "_Indexed")
val indexer = new StringIndexer().setInputCols(stringColumns).setOutputCols(stringColumnsIndexed)
val indexed = indexer.fit(data).transform(data)
indexed.show(5)


//Кодируем категориальные признаки
import org.apache.spark.ml.feature.OneHotEncoder

val catColumns = stringColumnsIndexed.map(_ + "_Coded")
val encoder = new OneHotEncoder().setInputCols(stringColumnsIndexed).setOutputCols(catColumns)
val encoded = encoder.fit(indexed).transform(indexed)
encoded.show(5)

//Собираем признаки в вектор
val featureColumns = numericColumnsFinal ++ catColumns
val assembler = new VectorAssembler().setInputCols(featureColumns).setOutputCol("features")
val assembled = assembler.transform(encoded)
assembled.show(5, truncate = false)

assembled.select("features").show(5, truncate = false)


//Нормализация
import org.apache.spark.ml.feature.MinMaxScaler

val scaler = new MinMaxScaler().setInputCol("features").setOutputCol("scaledFeatures")
val scaled = scaler.fit(assembled).transform(assembled)
scaled.select("features", "scaledFeatures").show(5, truncate = false)


//Feature Selection (отбор признаков)
import org.apache.spark.ml.feature.UnivariateFeatureSelector

val selector = new UnivariateFeatureSelector().setFeatureType("continuous").setLabelType("categorical").setSelectionMode("percentile").setSelectionThreshold(0.75).setFeaturesCol("scaledFeatures").setLabelCol("target").setOutputCol("selectedFeatures")
val dataF = selector.fit(scaled).transform(scaled)
dataF.select("scaledFeatures", "selectedFeatures").show(5, truncate = false)



//Моделирование
//Обучающая и тестовая выборки

val tt: Array[Dataset[Row]] = dataF.randomSplit(Array(0.7, 0.3))
val training = tt(0)
val test = tt(1)
println(s"training\t${training.count}\ntest    \t${test.count}")

//Логистическая регрессия
import org.apache.spark.ml.classification.LogisticRegression

val lr = new LogisticRegression().setMaxIter(1000).setRegParam(0.2).setElasticNetParam(0.8).setFamily("binomial").setFeaturesCol("selectedFeatures").setLabelCol("target")
val lrModel = lr.fit(training)
println(s"Coefficients: ${lrModel.coefficients}\nIntercept: ${lrModel.intercept}")


//Training Summary
val trainingSummary = lrModel.binarySummary
println(s"accuracy: ${trainingSummary.accuracy}")
println(s"areaUnderROC: ${trainingSummary.areaUnderROC}")

//Проверяем модель на тестовой выборке
val predicted = lrModel.transform(test)
predicted.select("target", "rawPrediction", "probability", "prediction").show(10, truncate = false)

predicted.show(5)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val evaluator = new BinaryClassificationEvaluator().setLabelCol("target")
println(s"areaUnderROC: ${evaluator.evaluate(predicted)}\n")


//Confusion Matrix
val tp = predicted.filter(($"target" === 1) and ($"prediction" === 1)).count
val tn = predicted.filter(($"target" === 0) and ($"prediction" === 0)).count
val fp = predicted.filter(($"target" === 0) and ($"prediction" === 1)).count
val fn = predicted.filter(($"target" === 1) and ($"prediction" === 0)).count
println(s"Confusion Matrix:\n$tp\t $fp\n$fn\t$tn\n")


//Accuracy, Precision, Recall
val accuracy = (tp + tn) / (tp + tn + fp + fn).toDouble
val precision = tp / (tp + fp).toDouble
val recall = tp / (tp + fn).toDouble
println(s"Accuracy = $accuracy")
println(s"Precision = $precision")
println(s"Recall = $recall\n")


//Настраиваем модель (подбираем гиперпараметры)
import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}

val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.01, 0.1, 0.5)).addGrid(lr.fitIntercept).addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0)).build()
val trainValidationSplit = new TrainValidationSplit().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setTrainRatio(0.7).setParallelism(2)
val model = trainValidationSplit.fit(dataF)

model.bestModel.extractParamMap()

val bestML = model.bestModel


//Собираем всё вместе (Pipeline)
import org.apache.spark.ml.Pipeline

val pipeline = new Pipeline().setStages(Array(indexer, encoder, assembler, scaler, selector, bestML))

val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))
val pipelineModel = pipeline.fit(trainingData)


//Сохраняем модель
pipelineModel.write.overwrite().save(s"$basePath/Scala/pipelineModel")



//Spark ML Production
//Загружаем данные
//val basePath = "/spark"
val data = spark.read.option("header", "true").option("inferSchema","true").csv(s"$basePath/BankChurners.csv")

//Загружаем модель
import org.apache.spark.ml.PipelineModel
val model = PipelineModel.load(s"$basePath/Scala/pipelineModel")

//Вычисляем
val predicted = model.transform(data)
predicted.show(10)

//Проверяем результат
val tp = predicted.filter(($"Attrition_Flag" === "Attrited Customer") and ($"prediction" === 1)).count
val tn = predicted.filter(($"Attrition_Flag" === "Existing Customer") and ($"prediction" === 0)).count
val fp = predicted.filter(($"Attrition_Flag" === "Existing Customer") and ($"prediction" === 1)).count
val fn = predicted.filter(($"Attrition_Flag" === "Attrited Customer") and ($"prediction" === 0)).count
println(s"Confusion Matrix:\n$tp\t$fp\n$fn\t\t$tn\n")

val accuracy = (tp + tn) / (tp + tn + fp + fn).toDouble
val precision = tp / (tp + fp).toDouble
val recall = tp / (tp + fn).toDouble
println(s"Accuracy = $accuracy")
println(s"Precision = $precision")
println(s"Recall = $recall\n")

